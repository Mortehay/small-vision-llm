networks:
  local-ai-net: {}
  shared-ai-network:
    external: true
services:
  ollama-llm:
    image: ollama/ollama:latest
    container_name: ollama-llm
    environment:
      - OLLAMA_VULKAN=1
      - HF_TOKEN=${HF_TOKEN}
    privileged: true
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card1:/dev/dri/card1
    security_opt:
      - seccomp=unconfined
    group_add:
      - "${RENDER_GID:-992}"
      - "${VIDEO_GID:-44}"
    networks:
      shared-ai-network:
        aliases:
          - ollama.loc
      local-ai-net:
        aliases:
          - ollama.loc
    deploy:
      resources:
        limits:
          memory: 8G  # Adjust based on your physical RAM
        reservations:
          memory: 4G

    ports:
      - "11435:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    restart: unless-stopped

  # --- Automated Model Downloader ---
  ollama-llm-pull-models:
    image: ollama/ollama:latest
    container_name: ollama-setup
    volumes:
      - ./ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=ollama:11434
      - HF_TOKEN=${HF_TOKEN}
    entrypoint: /bin/sh
    command: >
      -c "sleep 10;
      for model in ${LLM_LIST}; do
        if ! ollama list | grep -q \"$$model\"; then
          echo \"Model $$model not found, pulling...\";
          ollama pull $$model;
        else
          echo \"Model $$model already exists. Skipping.\";
        fi;
      done"
    depends_on:
      - ollama-llm
    restart: on-failure

  
  # --- Web Interface ---
  open-webui-llm:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-llm
    ports:
      - "11090:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./webui_data:/app/backend/data
    restart: unless-stopped
    
  # --- Camera Streamer ---
  stream-cam:
    container_name: stream-cam
    image: jrottenberg/ffmpeg
    user: root
    privileged: true 
    networks:
      - local-ai-net
      - shared-ai-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - /dev:/dev
    devices:
      - /dev/video0:/dev/video0
    environment:
      - FRAME_RESOLUTION=${FRAME_RESOLUTION}
      - FRAME_RESOLUTION_CROP=${FRAME_RESOLUTION_CROP}
      - CAMERA_URL=${CAMERA_URL}
    entrypoint: >
      sh -c 'ffmpeg -f v4l2 -input_format mjpeg -framerate 30 -video_size 640x480 -i /dev/video0 
      -vcodec libx264 -preset ultrafast -tune zerolatency 
      -x264-params "keyint=30:min-keyint=30:scenecut=0"
      -f mpegts udp://stream_operations:55080?pkt_size=1316'

  stream-operations:
    build:
      context: .
      dockerfile: configs/stream-operations/Dockerfile
    container_name: stream_operations
    user: root
    networks:
      - local-ai-net
      - shared-ai-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CAMERA_URL=${CAMERA_URL}
      - FRAME_RESOLUTION=${FRAME_RESOLUTION}
      - FRAME_RESOLUTION_CROPED=${FRAME_RESOLUTION_CROPED}
      - HSA_OVERRIDE_GFX_VERSION=10.3.0
      - LLM_LIST=${LLM_LIST}
      - OLLAMA_API_URL=${OLLAMA_API_URL}
    volumes:
      - /dev:/dev
      - ./scripts:/app/scripts
      - ./logs:/app/logs
    # Pass the specific script name as an argument
    entrypoint: ["tail", "-f", "/dev/null"]

